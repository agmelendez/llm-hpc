<!DOCTYPE html>
<html lang="es">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta http-equiv="X-UA-Compatible" content="ie=edge">
    <title>LLM-HPC Project - Portal de Informaci√≥n</title>
    <link rel="stylesheet" href="css/styles.css">
</head>
<body>
    <!-- Header con navegaci√≥n -->
    <header class="main-header">
        <div class="container">
            <div class="header-content">
                <div class="logo">
                    <h1>üß† LLM-HPC Project</h1>
                    <p class="tagline">Fine-Tuning de LLaMA 3.2 para Espa√±ol Latinoamericano</p>
                </div>
                <nav class="main-nav">
                    <a href="#overview" class="nav-link">Resumen</a>
                    <a href="#technical" class="nav-link">T√©cnico</a>
                    <a href="#results" class="nav-link">Resultados</a>
                    <a href="#infrastructure" class="nav-link">Infraestructura</a>
                    <a href="#methodology" class="nav-link">Metodolog√≠a</a>
                    <a href="#collaboration" class="nav-link">Colaboraci√≥n</a>
                </nav>
                <div class="user-info">
                    <span id="user-display">üë§ <span id="username"></span></span>
                    <button id="logout-btn" class="btn-secondary">Cerrar Sesi√≥n</button>
                </div>
            </div>
        </div>
    </header>

    <!-- Banner principal -->
    <section class="hero-banner">
        <div class="container">
            <h2 class="hero-title">Fine-Tuning Eficiente de LLaMA 3.2 (1B)</h2>
            <p class="hero-subtitle">con QLoRA (4-bit) y Unsloth en Infraestructura HPC-UCR</p>
            <div class="hero-meta">
                <div class="meta-item">
                    <strong>Instituci√≥n:</strong> Universidad de Costa Rica (UCR) - CIOdD
                </div>
                <div class="meta-item">
                    <strong>Fecha:</strong> Enero 2025
                </div>
                <div class="meta-item">
                    <strong>Autores:</strong> Alison Lobo Salas, MSI. Agust√≠n G√≥mez Mel√©ndez
                </div>
            </div>
        </div>
    </section>

    <!-- Contenido principal -->
    <main class="main-content">
        <div class="container">

            <!-- Secci√≥n: Resumen General -->
            <section id="overview" class="content-section">
                <h2 class="section-title">üìã Resumen General</h2>

                <div class="card">
                    <h3>Abstract</h3>
                    <div id="project-abstract" class="loading">Cargando informaci√≥n del proyecto...</div>
                </div>

                <div class="card">
                    <h3>üéØ Contribuciones Principales</h3>
                    <ul id="key-contributions" class="contribution-list loading">
                        <li>Cargando contribuciones...</li>
                    </ul>
                </div>

                <div class="info-box info">
                    <strong>‚ÑπÔ∏è Contexto Regional:</strong> Este trabajo forma parte de una colaboraci√≥n con el proyecto Latam-GPT de CENIA (Chile) y contribuye al desarrollo de capacidades regionales en modelos de lenguaje contextualizados para Am√©rica Latina.
                </div>
            </section>

            <!-- Secci√≥n: Detalles T√©cnicos -->
            <section id="technical" class="content-section">
                <h2 class="section-title">‚öôÔ∏è Detalles T√©cnicos</h2>

                <div class="grid-2">
                    <div class="card">
                        <h3>ü§ñ Modelo Base</h3>
                        <div id="model-info" class="tech-details loading">
                            <p>Cargando informaci√≥n del modelo...</p>
                        </div>
                    </div>

                    <div class="card">
                        <h3>üîß Configuraci√≥n de Entrenamiento</h3>
                        <div id="training-info" class="tech-details loading">
                            <p>Cargando configuraci√≥n...</p>
                        </div>
                    </div>
                </div>

                <div class="info-box warning">
                    <strong>‚ö†Ô∏è Nota T√©cnica:</strong> QLoRA (Quantized Low-Rank Adaptation) combina cuantizaci√≥n de 4 bits con adaptadores LoRA, reduciendo dr√°sticamente los requisitos de memoria GPU mientras mantiene la calidad del entrenamiento.
                </div>
            </section>

            <!-- Secci√≥n: Resultados y M√©tricas -->
            <section id="results" class="content-section">
                <h2 class="section-title">üìä Resultados y M√©tricas</h2>

                <div class="card">
                    <h3>üìà Resumen de M√©tricas</h3>
                    <div id="metrics-summary" class="metrics-grid loading">
                        <p>Cargando m√©tricas...</p>
                    </div>
                </div>

                <div class="card">
                    <h3>üìâ Evoluci√≥n por √âpoca</h3>
                    <div id="epoch-progress" class="loading">
                        <p>Cargando progreso...</p>
                    </div>
                </div>

                <div class="info-box success">
                    <strong>‚úÖ Resultado Destacado:</strong> La perplejidad se redujo un 74.8% (de 21.74 a 5.47), indicando una mejora sustancial en la capacidad del modelo para modelar secuencias en espa√±ol latinoamericano.
                </div>
            </section>

            <!-- Secci√≥n: Infraestructura -->
            <section id="infrastructure" class="content-section">
                <h2 class="section-title">üñ•Ô∏è Infraestructura HPC-UCR</h2>

                <div class="grid-2">
                    <div class="card">
                        <h3>üíª Hardware</h3>
                        <div id="hardware-info" class="loading">
                            <p>Cargando informaci√≥n de hardware...</p>
                        </div>
                    </div>

                    <div class="card">
                        <h3>üì¶ Software</h3>
                        <div id="software-info" class="loading">
                            <p>Cargando stack de software...</p>
                        </div>
                    </div>
                </div>

                <div class="card">
                    <h3>‚è±Ô∏è Costos Computacionales</h3>
                    <div id="computational-costs" class="tech-details">
                        <ul>
                            <li><strong>Tiempo total de entrenamiento:</strong> ~18 horas (3 bloques √ó 6h)</li>
                            <li><strong>GPU utilizada:</strong> NVIDIA A100 80GB</li>
                            <li><strong>Uso de VRAM:</strong> ~35-40 GB (picos)</li>
                            <li><strong>Sistema de colas:</strong> SLURM con checkpoints autom√°ticos</li>
                            <li><strong>Inferencia:</strong> 0.8-1.2 segundos por instrucci√≥n</li>
                        </ul>
                    </div>
                </div>
            </section>

            <!-- Secci√≥n: Metodolog√≠a -->
            <section id="methodology" class="content-section">
                <h2 class="section-title">üî¨ Metodolog√≠a</h2>

                <div id="methodology-content" class="loading">
                    <p>Cargando metodolog√≠a...</p>
                </div>

                <div class="card">
                    <h3>üé≤ Reproducibilidad</h3>
                    <ul>
                        <li>‚úÖ Semillas aleatorias fijas (seed=42)</li>
                        <li>‚úÖ Determinismo en PyTorch activado</li>
                        <li>‚úÖ Checkpointing cada 200 pasos</li>
                        <li>‚úÖ Evaluaci√≥n sincronizada cada 200 pasos</li>
                        <li>‚úÖ C√≥digo abierto en GitHub</li>
                    </ul>
                </div>

                <div class="info-box info">
                    <strong>üîó Repositorio:</strong> Todo el c√≥digo, scripts y documentaci√≥n est√°n disponibles p√∫blicamente en <a href="https://github.com/Alison-Lobo/llama32_qlora" target="_blank">GitHub</a>
                </div>
            </section>

            <!-- Secci√≥n: Colaboraci√≥n Latam-GPT -->
            <section id="collaboration" class="content-section">
                <h2 class="section-title">ü§ù Colaboraci√≥n con Latam-GPT</h2>

                <div id="collaboration-content" class="loading">
                    <p>Cargando informaci√≥n de colaboraci√≥n...</p>
                </div>

                <div class="card">
                    <h3>üåé Impacto Regional</h3>
                    <div id="regional-impact" class="loading">
                        <p>Cargando impacto...</p>
                    </div>
                </div>

                <div class="info-box success">
                    <strong>üöÄ Visi√≥n:</strong> Este proyecto sienta bases s√≥lidas para el desarrollo continuo de capacidades en modelos de lenguaje para espa√±ol latinoamericano, demostrando que la investigaci√≥n competitiva en LLMs es alcanzable con recursos limitados mediante metodolog√≠as inteligentes y colaboraci√≥n regional.
                </div>
            </section>

            <!-- Secci√≥n: Limitaciones y Trabajo Futuro -->
            <section id="future-work" class="content-section">
                <h2 class="section-title">üîÆ Limitaciones y Trabajo Futuro</h2>

                <div class="grid-2">
                    <div class="card">
                        <h3>‚ö†Ô∏è Limitaciones Actuales</h3>
                        <ul>
                            <li><strong>Evaluaci√≥n:</strong> M√©tricas intr√≠nsecas √∫nicamente (loss, perplejidad)</li>
                            <li><strong>Dataset:</strong> No p√∫blico por restricciones de privacidad</li>
                            <li><strong>Modelo:</strong> LLaMA 1B es relativamente peque√±o</li>
                            <li><strong>Benchmarks:</strong> Falta evaluaci√≥n en benchmarks est√°ndar de espa√±ol</li>
                            <li><strong>Evaluaci√≥n humana:</strong> No se realiz√≥ evaluaci√≥n cualitativa sistem√°tica</li>
                        </ul>
                    </div>

                    <div class="card">
                        <h3>üéØ Direcciones Futuras</h3>
                        <ul>
                            <li>Evaluar en benchmarks est√°ndar (BETO, Spanish SQuAD)</li>
                            <li>Implementar evaluaci√≥n humana sistem√°tica</li>
                            <li>Explorar modelos m√°s grandes (3B, 7B, 11B)</li>
                            <li>Aumentar diversidad del dataset</li>
                            <li>Entrenamiento multi-GPU distribuido</li>
                            <li>Colaboraci√≥n ampliada con Latam-GPT</li>
                        </ul>
                    </div>
                </div>
            </section>

            <!-- Secci√≥n: Contacto y Referencias -->
            <section id="contact" class="content-section">
                <h2 class="section-title">üì¨ Contacto y Referencias</h2>

                <div class="grid-2">
                    <div class="card">
                        <h3>üë• Autores</h3>
                        <div class="contact-info">
                            <div class="contact-item">
                                <strong>Alison Lobo Salas</strong><br>
                                Universidad de Costa Rica (UCR), CIOdD<br>
                                üìß alison.lobo@ucr.ac.cr
                            </div>
                            <div class="contact-item">
                                <strong>MSI. Agust√≠n G√≥mez Mel√©ndez</strong><br>
                                CIOdD--UCR<br>
                                üìß agustin.gomez@ucr.ac.cr
                            </div>
                        </div>
                    </div>

                    <div class="card">
                        <h3>üîó Enlaces</h3>
                        <ul>
                            <li><a href="https://github.com/Alison-Lobo/llama32_qlora" target="_blank">üìÇ Repositorio GitHub</a></li>
                            <li><a href="https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct" target="_blank">ü§ó LLaMA 3.2 Model Card</a></li>
                            <li><a href="https://cenia.cl/" target="_blank">üåé Proyecto Latam-GPT (CENIA)</a></li>
                            <li><a href="https://github.com/unslothai/unsloth" target="_blank">‚ö° Unsloth</a></li>
                        </ul>
                    </div>
                </div>
            </section>

        </div>
    </main>

    <!-- Footer -->
    <footer class="main-footer">
        <div class="container">
            <p>
                <strong>LLM-HPC Project</strong> - Fine-Tuning Eficiente de LLaMA 3.2 (1B) para Espa√±ol Latinoamericano<br>
                Universidad de Costa Rica (UCR) - Centro de Investigaci√≥n en Opini√≥n y Datos (CIOdD)<br>
                En colaboraci√≥n con Latam-GPT (CENIA, Chile) | Enero 2025
            </p>
            <p class="security-badge">
                üîí Portal seguro con autenticaci√≥n JWT, protecci√≥n CSRF y rate limiting
            </p>
        </div>
    </footer>

    <script src="js/app.js"></script>
</body>
</html>
